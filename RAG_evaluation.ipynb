{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start evaluating my chat assistant's question answering ability. Below are the validations I'm planning.\n",
    "1. Is the answer relevant to the question?\n",
    "2. Is the answer has all the important points taken from my resume relevant to the question asked?\n",
    "3. Are the contexts retrieved from vector db is the most matched chunks with my question?\n",
    "4. Is the answer relevant to the context ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I tried with [Non-LLM Evaluation techniques](#non_llm_evaluations). Later, I used [LLM Evaluation techniques](#llm_evaluations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting response from the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Pinecone as pc_vector\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "username = 'hari'\n",
    "\n",
    "def configure_retriever(index_name):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v1\")\n",
    "\n",
    "    docsearch = pc_vector.from_existing_index(index_name, embeddings)\n",
    "    retriever = docsearch.as_retriever(search_type=\"mmr\")\n",
    "    return retriever\n",
    "\n",
    "db_path = 'local_sqlite_db.db'\n",
    "msgs = SQLChatMessageHistory(\n",
    "    session_id=username,\n",
    "    connection=\"sqlite:///\" + db_path  # This is the SQLite connection string\n",
    ")\n",
    "\n",
    "# index_name = \"my-portfolio\"\n",
    "index_name = 'test-my-portfolio'\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=msgs, return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\", openai_api_key=openai_api_key, temperature=0, streaming=True\n",
    "    )\n",
    "\n",
    "system_prompt = \"You are A CHAT ASSISTANT even with or without context and you are NOT Haripriya. You are supposed to answer the questions asked only about Haripriya and you are NOT Haripriya. Use the following pieces of retrieved context to answer the question.\\n\\n{context}\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "]\n",
    ")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, retriever=configure_retriever(index_name), memory=memory, verbose=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    ")\n",
    "\n",
    "user_query = 'Give educational background of Haripriya'\n",
    "user_query += ' Brief it in at most 35 words or the count limit given in the previous sentence.'\n",
    "response = qa_chain.invoke({\"question\":user_query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output was huge, I took a snapshot of it to get an idea of which format I was looking at. So, here is the snap [Old Context Format's Image](images/old_context_format_pdf.png). You can see that there are so many spacings. This happens in the same way when we copy texts from pdf directly and paste it. Even though there were no spaces in the pdf, the pasting technique introduces it. The same scenario is happening in the pdf conversion to embedding as well. This fogs the actual context retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-LLM Evaluations\n",
    "<a id='non_llm_evaluations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLEU Score: Compares the generated text to reference texts, assessing the overlap of n-grams (sequences of words).\n",
    "- ROUGE Score: Similar to BLEU, but focuses on recall, measuring how much of the reference content is captured in the generated text.\n",
    "- METEOR: Considers synonyms and paraphrases, aiming to better align with human judgment.\n",
    "- BERTScore: Evaluates text by comparing the similarity of contextual embeddings, focusing on meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # For BLEU and other metrics (if needed)\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "\n",
    "# 1. BLEU Score\n",
    "def calculate_bleu(generated, references):\n",
    "    \"\"\"Calculates BLEU score.\"\"\"\n",
    "    smoothing = SmoothingFunction().method4  # Choose a smoothing function\n",
    "    bleu_scores = []\n",
    "    for i in range(len(generated)):\n",
    "        score = sentence_bleu(references[i], generated[i].split(), smoothing_function=smoothing)\n",
    "        bleu_scores.append(score)\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "# 2. ROUGE Score\n",
    "def calculate_rouge(generated, references):\n",
    "    \"\"\"Calculates ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L).\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_l_scores = []\n",
    "\n",
    "    for i in range(len(generated)):\n",
    "        scores = scorer.score(references[i][0], generated[i]) # reference is a list of sentences, we take the first one\n",
    "        rouge_1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge_2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rouge_l_scores.append(scores['rougeL'].fmeasure)\n",
    "    return np.mean(rouge_1_scores), np.mean(rouge_2_scores), np.mean(rouge_l_scores)\n",
    "\n",
    "# 3. BERTScore\n",
    "def calculate_bertscore(generated, references):\n",
    "    \"\"\"Calculates BERTScore.\"\"\"\n",
    "    P, R, F1 = score(generated, [ref[0] for ref in references], lang=\"en\")  # reference is a list of sentences, we take the first one\n",
    "    return np.mean(P.cpu().numpy()), np.mean(R.cpu().numpy()), np.mean(F1.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clearing out session when required\n",
    "msgs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.8457125425338745\n",
      "BERTScore Recall: 0.861638069152832\n",
      "BERTScore F1: 0.8536010384559631\n"
     ]
    }
   ],
   "source": [
    "reference_texts = [['Haripriya had good scores in academics during school and studied Electronics and Communication Engineering. She also is studying micromasters in Statistics and Data science']]\n",
    "generated_texts = ['Haripriya excelled academically, achieving School First in both SSLC and Higher Secondary School with 97% marks, and District First in the Sri Ramanujan Maths Aptitude Test and Second in a Maths Quiz during HSC.']\n",
    "\n",
    "bert_p, bert_r, bert_f1 = calculate_bertscore(generated_texts, reference_texts)\n",
    "print(f\"BERTScore Precision: {bert_p}\")\n",
    "print(f\"BERTScore Recall: {bert_r}\")\n",
    "print(f\"BERTScore F1: {bert_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is low. I'm expecting a score of 90 and above. I'll be re-creating the index by modifying the pdf and chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified the chunk size to 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, retriever=configure_retriever(index_name), memory=memory, verbose=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    ")\n",
    "\n",
    "user_query = 'Give educational background of Haripriya'\n",
    "user_query += ' Brief it in at most 35 words or the count limit given in the previous sentence.'\n",
    "response = qa_chain.invoke({\"question\":user_query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.8500571846961975\n",
      "BERTScore Recall: 0.8697115182876587\n",
      "BERTScore F1: 0.8597720265388489\n"
     ]
    }
   ],
   "source": [
    "reference_texts = [['Haripriya had good scores in academics during school and studied Electronics and Communication Engineering. She also is studying micromasters in Statistics and Data science']]\n",
    "\n",
    "generated_texts = ['Haripriya Rajendran has completed a Micromasters in ML with Python, Google Advanced Data Analytics and Data Analytics Professional Certificates, and a Machine Learning course from Stanford University, among other certifications.']\n",
    "\n",
    "bert_p, bert_r, bert_f1 = calculate_bertscore(generated_texts, reference_texts)\n",
    "print(f\"BERTScore Precision: {bert_p}\")\n",
    "print(f\"BERTScore Recall: {bert_r}\")\n",
    "print(f\"BERTScore F1: {bert_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still I did not get the score I was expecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified the whole embedding model back to the older L6 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, retriever=configure_retriever(index_name), memory=memory, verbose=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    ")\n",
    "\n",
    "user_query = 'Give educational background of Haripriya'\n",
    "user_query += ' Brief it in at most 35 words or the count limit given in the previous sentence.'\n",
    "response = qa_chain.invoke({\"question\":user_query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.857337474822998\n",
      "BERTScore Recall: 0.861737847328186\n",
      "BERTScore F1: 0.8595320582389832\n"
     ]
    }
   ],
   "source": [
    "reference_texts = [['Haripriya had good scores in academics during school and studied Electronics and Communication Engineering. She also is studying micromasters in Statistics and Data science']]\n",
    "\n",
    "generated_texts = ['Haripriya excelled academically, achieving school first in both SSLC and Higher Secondary with 97% marks, and district first in the Sri Ramanujan Maths aptitude test, along with second place in a Maths quiz.']\n",
    "\n",
    "bert_p, bert_r, bert_f1 = calculate_bertscore(generated_texts, reference_texts)\n",
    "print(f\"BERTScore Precision: {bert_p}\")\n",
    "print(f\"BERTScore Recall: {bert_r}\")\n",
    "print(f\"BERTScore F1: {bert_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is still low. Let's use some actual LLM-RAG's evaluation  metrics and see what might be the issues with context, input or output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics using Deepval - LLM Evaluation\n",
    "<a id='llm_evaluations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken the input, output and the contexts from the above queries that was ran with langchain. It was in the output cells. I took that and stored in another local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import contexts_for_testing\n",
    "contexts = contexts_for_testing.context_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1136, 816, 493, 1742]\n"
     ]
    }
   ],
   "source": [
    "print([len(context) for context in contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac0d0f477e9428880fda3630d91164d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4344f139628a4a1e9c0d659975fff345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualRecall :  0.0\n",
      "The score is 0.00 because none of the sentences in the expected output align with any of the nodes in the retrieval context, as there are no relevant quotes to support Haripriya's academic achievements or her current studies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aec1be8446e406399b375d9040e4930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualRelevancy :  0.3448275862068966\n",
      "The score is 0.34 because while there are relevant educational achievements mentioned, such as 'DISTRICT FIRST in Sri Ramanujan Maths aptitude test' and various certificates from prestigious institutions like 'MITx' and 'Stanford University', the majority of the retrieval context is focused on technical skills and work experiences that do not pertain to Haripriya's educational background.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualPrecision :  0.3333333333333333\n",
      "The score is 0.33 because while there is one relevant node that provides direct information about Haripriya's educational background, the higher-ranked nodes predominantly contain irrelevant content. Specifically, the first node ranks highest but states, \"The first document does not mention Haripriya's academic background or any relevant details about her education,\" thus contributing to the lower score. Furthermore, the second and fourth nodes also do not provide pertinent information related to the request, confirming that relevant nodes are not ranked high enough.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric, ContextualRelevancyMetric, ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Actual output from your LLM application\n",
    "actual_output = \"Haripriya Rajendran excelled academically, achieving school first in both SSLC and Higher Secondary with 97% marks, and district first in the Sri Ramanujan Maths aptitude test, alongside various professional certifications in data science and analytics.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"'Haripriya had good scores in academics during school and studied Electronics and Communication Engineering. She also is studying micromasters in Statistics and Data science'\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = contexts\n",
    "\n",
    "contextualRecall = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Give educational background of Haripriya\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "contextualRelevancy = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextualPrecision = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextualRecall.measure(test_case)\n",
    "print(\"contextualRecall : \", contextualRecall.score)\n",
    "print(contextualRecall.reason)\n",
    "\n",
    "contextualRelevancy.measure(test_case)\n",
    "print(\"contextualRelevancy : \", contextualRelevancy.score)\n",
    "print(contextualRelevancy.reason)\n",
    "\n",
    "contextualPrecision.measure(test_case)\n",
    "print(\"contextualPrecision : \", contextualPrecision.score)\n",
    "print(contextualPrecision.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47e7032f40c4f749335de0fe34b84de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness : 0.75\n",
      "The score is 0.75 because while the actual output accurately conveys Haripriya Rajendran's top achievements in her classes, it incorrectly interprets 'first in academics' as 'school first'. This slight misalignment leads to a reduction in faithfulness to the retrieval context.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "faithfulness.measure(test_case)\n",
    "print(\"faithfulness :\", faithfulness.score)\n",
    "print(faithfulness.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The contextual recall is just 0 telling that none of the contexts actually aligns with the expected output\n",
    "- The contextual relevancy is also very low and explains why there is a very low score in a more human understandable way. This checks the relevancy between the input and the context \n",
    "- The contexual precision is also low because only one of the four documents is somewhat similar to the input and is also not ranked at the top. There is an irrelevant document at the top\n",
    "- faithfulness has a good score because the actual output is faithful to the context. The LLM actually answers properly with whatever context it has.\n",
    "\n",
    "So, from this we can understand that the retrieved context is actually the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I figured that the context given is not as expected and the context format is also not proper since it's extracted from a formatted pdf. So, I tried to populate the pinecone index with Docxloaders instead of pypdfloader and used a word document. Now this document is used to create vectors and then used them as context. Hopefully, there won't be any formatting issue this time and the context is proper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, retriever=configure_retriever(index_name), memory=memory, verbose=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    ")\n",
    "\n",
    "user_query = 'Give educational background of Haripriya'\n",
    "user_query += ' Brief it in at most 35 words or the count limit given in the previous sentence.'\n",
    "response = qa_chain.invoke({\"question\":user_query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output was huge, I took a snapshot of it to get an idea of which format of context I was looking at. \n",
    "\n",
    "So, here are the snaps [New Context Format's Image](images/new_context_format_word_1.png), [New Context Format's Image 2](images/new_context_format_word_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.8738595843315125\n",
      "BERTScore Recall: 0.9114338159561157\n",
      "BERTScore F1: 0.8922513127326965\n"
     ]
    }
   ],
   "source": [
    "reference_texts = [['Haripriya had good scores in academics during school and studied Electronics and Communication Engineering. She also is studying micromasters in Statistics and Data science']]\n",
    "\n",
    "generated_texts = [\"Haripriya holds a Bachelor's degree in Electronics and Communication Engineering from Sri Shakthi Institute of Engineering and Technology, Coimbatore, with a CGPA of 8.44, and is pursuing a Micromasters in Statistics and Data Science.\"]\n",
    "\n",
    "bert_p, bert_r, bert_f1 = calculate_bertscore(generated_texts, reference_texts)\n",
    "print(f\"BERTScore Precision: {bert_p}\")\n",
    "print(f\"BERTScore Recall: {bert_r}\")\n",
    "print(f\"BERTScore F1: {bert_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better recall score and much better response as well. Earlier there was no information stating about my Engineering degree which I was expecting the most. Now it's better. We will try few other responses as well.\n",
    "\n",
    "As you can see, the context is of proper format which made this happen. Let's try the LLM evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contexts_for_testing\n",
    "contexts = contexts_for_testing.context_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1979, 1146, 1929, 1876]\n"
     ]
    }
   ],
   "source": [
    "print([len(context) for context in contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245b2a762c0f43fa9c684d6263920ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdaefc7ac5d451d8419ba1b92f83128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualRecall :  1.0\n",
      "The score is 1.00 because the expected output aligns perfectly with the details from the nodes in the retrieval context, specifically mentioning 'Electronics and Communication Engineering' from node 9 and the 'micromasters in Statistics and Data Science' from node 1 under 'EDUCATION'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2def4bef8681447ba284b8d0a2925928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualRelevancy :  0.4186046511627907\n",
      "The score is 0.42 because while there are relevant educational achievements listed, such as 'Bachelors in Electronics and Communication Engineering - 2014 - 2018' and various certifications like 'Google Advanced Data Analytics Professional Certificate', the majority of the context focuses on job tasks and non-educational qualifications, as highlighted in the reasons for irrelevancy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualPrecision :  0.5833333333333333\n",
      "The score is 0.58 because although there are relevant nodes that provide details about Haripriya's education, two irrelevant nodes rank higher than the last relevant one. Specifically, the first node detailing data preparation is ranked first and states, 'Performed Retro Data preparation and Analysis on new data products' which does not mention Haripriya's educational background. This lowers the score despite the presence of two relevant nodes ranked second and third, which effectively address her academic qualifications.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric, ContextualRelevancyMetric, ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Actual output from your LLM application\n",
    "actual_output = \"Haripriya Rajendran excelled academically, achieving school first in both SSLC and Higher Secondary with 97% marks, and district first in the Sri Ramanujan Maths aptitude test, alongside various professional certifications in data science and analytics.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"'Haripriya had good scores in academics during school and studied Electronics and Communication Engineering. She also is studying micromasters in Statistics and Data science'\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = contexts\n",
    "\n",
    "contextualRecall = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Give educational background of Haripriya\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "contextualRelevancy = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextualPrecision = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextualRecall.measure(test_case)\n",
    "print(\"contextualRecall : \", contextualRecall.score)\n",
    "print(contextualRecall.reason)\n",
    "\n",
    "contextualRelevancy.measure(test_case)\n",
    "print(\"contextualRelevancy : \", contextualRelevancy.score)\n",
    "print(contextualRelevancy.reason)\n",
    "\n",
    "contextualPrecision.measure(test_case)\n",
    "print(\"contextualPrecision : \", contextualPrecision.score)\n",
    "print(contextualPrecision.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The contextual recall is the perfect score now that means atleast one of the contexts actually aligns with the expected output\n",
    "- The contextual relevancy is still low but better than previous because even though we have the context that we need, other contexts are not relevant to the input\n",
    "- The contexual precision is still low because there are relevant documents but is not ranked at the top. There is an irrelevant document at the top\n",
    "\n",
    "So far so good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, retriever=configure_retriever(index_name), memory=memory, verbose=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    ")\n",
    "\n",
    "user_query = 'give a summary on her Generative AI projects'\n",
    "user_query += ' Brief it in at most 35 words or the count limit given in the previous sentence.'\n",
    "response = qa_chain.invoke({\"question\":user_query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.8725282549858093\n",
      "BERTScore Recall: 0.874882698059082\n",
      "BERTScore F1: 0.8737038373947144\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "reference_texts = [['Haripriya has worked on her portfolio project on creating a chat assistant about her using RAG with generativeAI and langchain. She also has created a weather API agent using streamlit']]\n",
    "\n",
    "generated_texts = [\"Haripriya specializes in Generative AI, utilizing LangChain for RAG, tool calling with Agents, and fine-tuning Gemma models, enhancing predictive capabilities and automating processes in data science applications.\"]\n",
    "\n",
    "bert_p, bert_r, bert_f1 = calculate_bertscore(generated_texts, reference_texts)\n",
    "print(f\"BERTScore Precision: {bert_p}\")\n",
    "print(f\"BERTScore Recall: {bert_r}\")\n",
    "print(f\"BERTScore F1: {bert_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1509, 1929, 1876, 1146]\n"
     ]
    }
   ],
   "source": [
    "import contexts_for_testing\n",
    "contexts = contexts_for_testing.context_3\n",
    "print([len(context) for context in contexts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706bcf1188984c7e81745a85f86b1b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dcea6fa54e4ae2810fc8e607674829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualRecall :  1.0\n",
      "The score is 1.00 because all elements of the expected output perfectly align with the provided context nodes, with the chat assistant work linking to node 5 and the weather API creation linked to node 1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e9cb387f5a49e797e3a0a2ecda3ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualRelevancy :  0.12903225806451613\n",
      "The score is 0.13 because, despite the presence of relevant statements like 'Lead Data Scientist with GenerativeAI' and 'expertise in using GenAI models,' the majority of the context focuses on unrelated qualifications and general skills rather than summarizing specific Generative AI projects.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextualPrecision :  1.0\n",
      "The score is 1.00 because all relevant nodes are ranked higher than the irrelevant nodes. The first node highlights expertise in 'using GenAI models for RAG using LangChain', making it directly pertinent to Generative AI projects. Similarly, the second node presents a specific Generative AI portfolio project, the 'langchain-weather-tool-calling'. In contrast, the third and fourth nodes discuss themes unrelated to Generative AI, such as model performance evaluation and course certifications, allowing for a clear distinction in relevance.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric, ContextualRelevancyMetric, ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Actual output from your LLM application\n",
    "actual_output = \"Haripriya specializes in Generative AI, utilizing LangChain for RAG, tool calling with Agents, and fine-tuning Gemma models, enhancing predictive capabilities and automating processes in data science applications.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = 'Haripriya has worked on her portfolio project on creating a chat assistant about her using RAG with generativeAI and langchain. She also has created a weather API agent using streamlit'\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = contexts\n",
    "\n",
    "contextualRecall = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"give a summary on her Generative AI projects\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "contextualRelevancy = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextualPrecision = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextualRecall.measure(test_case)\n",
    "print(\"contextualRecall : \", contextualRecall.score)\n",
    "print(contextualRecall.reason)\n",
    "\n",
    "contextualRelevancy.measure(test_case)\n",
    "print(\"contextualRelevancy : \", contextualRelevancy.score)\n",
    "print(contextualRelevancy.reason)\n",
    "\n",
    "contextualPrecision.measure(test_case)\n",
    "print(\"contextualPrecision : \", contextualPrecision.score)\n",
    "print(contextualPrecision.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The contextual recall is the perfect score now that means atleast one of the contexts actually aligns with the expected output\n",
    "- The contextual relevancy is very low because even though we have the context that we need, other contexts are not relevant to the input\n",
    "- The contexual precision is still low because there are relevant documents but is not ranked at the top. There is an irrelevant document at the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot increase contextual relevancy for now, but increasing contextual recall was the major goal and we achieved it! I would consider this as a success, atleast for this project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
